---
title: "End-to-end data analysis pipeline"
subtitle: 'An visualization journey'
author: "<br>Yikai Zhao&nbsp;&nbsp;&nbsp;`r anicon::faa('twitter', animate='vertical', rtext='&nbsp;@zhao_yikai')`"
institute: "Looking for a full time position as Data scientist"
date: "25-Oct-2019 &nbsp;&nbsp; `r anicon::faa('link', animate='ring', rtext='&nbsp;ykzhao.com', color='white')`"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    lib_dir: libs
    css: ["assets/shinobi.css", "assets/ninjutsu.css", "assets/ninpo.css", "assets/default.css"]
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

class: center, middle

<img src="images/data_scientist.png" width="25%">


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(dpi = 300,  out.width = "80%", message = F, warning = F) # this is set for the aesthetics of the 
# flipplot remember to add out.width to all other plots


```



---


class: split-60 white 

.column.bg-main1[.content.vmiddle[
# Overview
<br>
### Data visualization is important to

* #### draw insights amid the data analysis
* #### convey those insights to customers & decision makers


### This powerpoint aims to use .orange[visualizations] to showcase the process of data analysis from data cleaning to final model interpretation

### I will use the House Prices dataset from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)


### The information about this dataset could be glanced from right


]]



.column.bg-main4[.content.vmiddle.center[
<center>
```{r echo = FALSE, message=FALSE, warning=FALSE }
library(knitr)
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(corrplot)
# library(caret)
# library(gridExtra)
# library(scales)
# library(Rmisc)
library(ggrepel)
library(plotly)
library(randomForest)
# library(psych)
# library(xgboost)
train <- read.csv("~/Data_product_pipeline/kunoichi/data/train_house.csv", stringsAsFactors = F)
test <- read.csv("~/Data_product_pipeline/kunoichi/data/test_house.csv", stringsAsFactors = F)
test_labels <- test$Id
test$Id <- NULL
train$Id <- NULL
test$SalePrice <- NA
all <- rbind(train, test)
```

```{r plot_themes, echo=F, message=F, warning=F}
yikai_labels <- list(
   labs(subtitle = "Data Source: House Prices Competition in Kaggle"),
   labs(caption = "Vis: @zhao_yikai with ggplot")
             )

yikai_palette  <- c("#FFDB6D", "#C4961A", "#F4EDCA", 
                "#D16103", "#C3D7A4", "#52854C", "#4E84C4", "#293352")
yikai_themes = theme(
  plot.subtitle = element_text(color = 'grey50'),
  panel.border = element_blank(),
  plot.title = element_text(face = "bold", size = 16),
  panel.grid.minor = element_blank(),
  axis.line.x = element_line(color = 'darkgrey', size = 0.5),
  plot.title.position = "panel", 
  plot.caption.position =  "plot") #NEW parameter
```

```{r, eval = F}
# The dimention of the data are
dim(train)
dim(test)
```

```{r, eval=F}
[1] 1460   80
[1] 1459   80
```

### There are 79 predictors and we will use them to predict our response variable .teal[SalePrice] of houses. 
]]

---
class: bg-main1 center middle hide-slide-number

.reveal-text[.pad1[
.font4[There will be three major parts
]
]]


---
layout: true

class: split-three 

.column.bg-main1[.content[
#  Data exploration and manipulation 
### This step involves dealing of .orange[missing data] and .teal[outliers]:
  + ### missing at random or missing with pattern(s)
  + ### impute or discard them
  
### View the distribution of variables and their relaitonships
### Deal with skewed variables
### Feature engineering 

]]
.column.bg-main1[.content[
# Predicative modeling

## Choose the appropriate modeling architecture(s)
## .orange[hyperparamerter tuning] 
## .teal[ensembling] the models for better generalibility
]]
.column.bg-main1[.content[
# Results interpretation

## Interpretable machine learning
## .orange[Shapley] values to attribute contributions to variables
## View interactions between variables
]]

---

class: gray-col2 gray-col3
count: false

---

class: gray-col3
count: false

---

count: false

---
layout: false

class: bg-main1, center, middle

# Part 1: Data exploration and manipulation

---

layout: false
class: split-50 
.column.bg-main1[.content.vmiddle[

# Which numerical variables are important to explore first

## We could find variables with a correlation of at least 0.5 with our dependent variable .orange[SalePrice]

]]

.column.bg-main5[.content.vmiddle.center[

```{r, echo= F, out.width= '100%', warning=F, message= F}
numericVars <- which(sapply(all, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on

all_numVar <- all[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs") #correlations of all numeric variables

#sort on decreasing correlations with SalePrice
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
 #select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
```

```{r, out.height= "40%" }
#sorted on decreasing correlations with SalePrice
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")
```


]]


---

class: split-50

.column.bg-main1[.content.vmiddle[
# We could visualize the relationship between .pink[GrLivArea] and .orange[SalePrice]
From the plot, the 524th and 1299th house are potential outliers that deserve special attention. We furter insepct the OverallQual of these two hourse

```{r, echo = F}
knitr::kable(all[c(524, 1299), c('SalePrice', 'GrLivArea', 'OverallQual', "Neighborhood")]
, format = "html")
```

These two houses are all of the highest quality however located at a "normal" .tile[neighborhood], which might explain the low price. Thus we keep these two houses in the dataset. 


]]

.column.bg-main5[.content.vmiddle.center[


```{r, out.width = "85%", echo = F}
p = ggplot(data=all[!is.na(all$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
        geom_point(color = ifelse(all$GrLivArea[!is.na(all$SalePrice)]>4500, "#E91E63", "grey50"), size = ifelse(all$GrLivArea[!is.na(all$SalePrice)]>4500, 4, 2)) + 
        geom_smooth(method = "lm", se=FALSE, color="black", lty = "dotted") +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = scales::comma) +
        geom_label_repel(aes(label = ifelse(all$GrLivArea[!is.na(all$SalePrice)]>4500, rownames(all), '')))+
        theme_bw()+
        theme(plot.subtitle = element_text(color = 'grey70'))+
        theme(panel.border = element_blank())+
        theme(plot.title = element_text(lineheight = .8, face = "bold", size = 16))+
        theme(panel.grid.minor = element_blank())+
        theme(plot.subtitle = element_text(color = 'darkgray'))+
        theme(axis.line.x = element_line(color = 'darkgrey', size = 0.5))+
        labs(title = "Relationship between SalesPrice and GrLivArea")+
        labs(subtitle = "Data Source: House Prices Competition in Kaggle")+
        labs(caption = "Vis: @zhao_yikai with ggplot")
p
```


]]

---
class: bg-main1, center, middle


###  In the next slide, we visualize the relationship between .yellow[OverallQual] and .pink[SalePrice] in steps.

# We could customize the visualization with great flexibility to the businuess needs. 

---

```{r, echo = FALSE, warning = F, message= F}

training_data = all[!is.na(all$SalePrice), ]

lb <- function(x) mean(x) - sd(x)
ub <- function(x) {

  return(mean(x) + sd(x))
}

source("https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R")


getPalette = colorRampPalette(RColorBrewer::brewer.pal(8, "RdBu"))(10)

sumld <- training_data %>% 
  dplyr::select(OverallQual, SalePrice) %>% 
  group_by(OverallQual) %>% 
  summarise_all(funs(mean, median, lower = lb, upper = ub))

source("https://raw.githubusercontent.com/EvaMaeRey/little_flipbooks_library/master/xaringan_reveal_parentheses_balanced.R")

```

```{r rainplot, eval = F, echo = F,  out.width = "100%", warning = F, message = F, dpi= 300}
# We could visualize the distribution
# of variable OverallQual in a rainplot
ggplot(data = training_data, 
  aes(x = factor(OverallQual), y = SalePrice, fill=factor(OverallQual))) +
  geom_flat_violin(position = position_nudge(x = .2, y = 0), trim = TRUE, alpha = .8, scale = "width") +
  geom_point(aes(y = SalePrice, color = factor(OverallQual)), 
             position = position_jitter(width = .15), size = .5, alpha = 1) +
  geom_boxplot(width = .1, outlier.shape = NA, alpha = 0.5) +
  geom_point(data = sumld, aes(x = factor(OverallQual), y = mean), 
             position = position_nudge(x = 0.3), size = 2.5) +
  geom_errorbar(data = sumld, aes(ymin = lower, ymax = upper, y = mean), 
                position = position_nudge(x = 0.3), width = 0)+
  expand_limits(x = 5.25) +
  guides(fill = FALSE, color = F) +
  scale_color_manual(values = getPalette) +
  scale_fill_manual(values = getPalette) +
  coord_trans(y = 'log2') +
  labs( y = 'SalePrice', x = 'OverallQual' ) +
  annotate("text", x = 4, y = 3e+05, label = c('outlier?'))+
  labs(title = "Relationship between SalesPrice and OverallQual")+
  labs(subtitle = "Data Source: House Prices Competition in Kaggle", 
       caption = "Vis: @zhao_yikai with ggplot")+
  theme_bw()+
  theme(plot.subtitle = element_text(color = 'grey50'))+
  theme(panel.border = element_blank())+
  theme(plot.title = element_text(lineheight = .8, face = "bold", size = 16))+
  theme(panel.grid.minor = element_blank())+
  theme(axis.line.x = element_line(color = 'darkgrey', size = 0.5))+
  scale_y_continuous(breaks= seq(0, 800000, by=100000), labels =scales::comma)
  
```


`r apply_reveal("rainplot")`

---

class: split-50  

.column.bg-main1[.content.vmiddle[

# Visualize the relaionship between .orange[Year] and .teal[SalePrice] by .pink[OverallQual]

The dotted line is roughly the time of the economic crisis happend at the end of 2017.
We could see clearly that the high quality (10) houses get influenced the most.


]]


.column.bg-main5[.content.vmiddle.center[
```{r, echo = F}
all %>%
  select(YrSold, OverallQual, SalePrice) %>%
  filter(!is.na(SalePrice)) %>%
  group_by(YrSold, OverallQual) %>%
  summarise(median_price = median(SalePrice), mean_price = mean(SalePrice)) -> year_by_quality_data

n_qual = paste("N = ", table(all$OverallQual))
x_location = rep(2009, 10)
y_location = c(rep(500000, 9), 200000)
anno_data = data.frame(n_qual = n_qual,
                       YrSold = x_location,
                       median_price = y_location,
                       OverallQual = c(1:10))
```



```{r, echo = F, out.width= "85%"}
ggplot(data = year_by_quality_data, aes(x = YrSold, y = median_price))+
  geom_line(aes(color = "Median"))+
  facet_wrap(~ OverallQual)+
  geom_line(data = year_by_quality_data, aes(x = YrSold, y = mean_price, color = "Mean"), )+
  scale_y_continuous(breaks = seq(0, 750000, by= 100000), 
                     labels = scales::comma)+
  geom_text(data = anno_data, aes(label = n_qual,
                                  group = NULL))+
  geom_vline(xintercept = 2007.8, linetype = "dotted") +
  theme_light()+
  yikai_labels+
  yikai_themes+
  scale_x_continuous(breaks = c(2007, 2008, 2009), labels = c("07", "08", "09"))+
  scale_y_continuous(breaks = c(100000, 200000, 400000, 600000), labels = c("100k", "200k", "400k", "600k"))+
  theme(panel.grid.major.x = element_blank())+
  theme(legend.position = c(0.8,0.1), legend.direction = "horizontal")+
  theme(legend.background = element_rect(color = "grey" ))+
  theme(panel.spacing.y =
          unit(1, "lines"))+
  labs(y = 'Mean and Median of SalePrice')+
  labs(x = 'Year')+
  labs(title = 'Change of price by Year by OverallQual')
```



]]


---

class: split-50  

.column.bg-main1[.content.vmiddle[

# Use .pink[principle components] of numerical variables to detect extreme obervations

]]

.column.bg-main5[.content.vmiddle[


```{r, echo = F}
DFnorm = readRDS("~/Data_product_pipeline/kunoichi/data/normed_numerical_vars.rds")
pcaOut      = prcomp(DFnorm,scale=F,center=F)
data.frame(pcaOut$x) %>%
ggplot() +
  geom_point(aes(x = PC1, y = PC2))
  


```


]]


---

class: split-50 

.column.bg-main1[.content.vmiddle[

# Which .orange[categorical variables] are important to explore

## We could find them through .teal[Random Forest] feature importance permutation test. 

]]

.column.bg-main5[.content.vmiddle.center[

```{r, echo = F, out.width = "90%" }
imp_DF = readRDS("~/Data_product_pipeline/kunoichi/data/RF_importance.RDS")


ggplot(imp_DF[1:10,], aes(x=reorder(Variables, MSE), y= MSE, label = round(MSE, 1))) +
  geom_segment( aes(x=reorder(Variables, MSE), xend=reorder(Variables, MSE), y=0, yend= MSE ), color=ifelse(imp_DF[1:10,]$Variables %in% c("Neighborhood", "MSSubClass"), "orange", "grey"), size=ifelse(imp_DF[1:10,]$Variables  %in% c("Neighborhood", "MSSubClass"), 1.5, 0.7) ) +
  geom_point( color=ifelse(imp_DF[1:10,]$Variables %in% c("Neighborhood", "MSSubClass"), "orange", "grey"), size=ifelse(imp_DF[1:10,]$Variables  %in% c("Neighborhood", "MSSubClass"), 9, 5) ) +
  #theme_ipsum() +
  coord_flip() +
  theme(
    legend.position="none"
  ) +
  geom_text(color = "white", size = ifelse(imp_DF[1:10,]$Variables %in% c("Neighborhood", "MSSubClass"), 3, 2))+
  xlab("") +
  ylab("Value of Y") +
  ggtitle("Feature importance via Random Forrest")+
  labs(x = 'Variables',y= '% increase MSE if variable is randomly permuted')+
  theme_bw()+
  yikai_labels +
  yikai_themes+
  ylim(0, 22)+
  theme(plot.title.position = "plot")+
  annotate("text", x=9, y=imp_DF[1:10,]$MSE[which(imp_DF[1:10,]$Variables=="Neighborhood")]*1.1, 
             label= paste('Important categorical variable'),
             color="orange", size=3, angle=0, fontface="bold", hjust=0)+
  annotate("text", x=6, y=imp_DF[1:10,]$MSE[which(imp_DF[1:10,]$Variables=="MSSubClass")]*1.2, 
           label= paste('Another important categorical variable'),
           color="orange", size=3, angle=0, fontface="bold", hjust=0)+
  theme(axis.title.x = element_text(hjust=-0.06))+
  theme(panel.grid.major.y = element_blank())
```



]]


---

class: split-50  

.column.bg-main1[.content.vmiddle[
# We could view the .pink[Neighborhood] variable in terms of the median .orange[SalePrice]

### The label denotes the count of houses in that neighborhood
### The dashed line is median SalePrice

]]

.column.bg-main5[.content.vmiddle.center[

```{r, out.width= "85%", echo = F}
ggplot(all[!is.na(all$SalePrice),], aes(x=Neighborhood, y=SalePrice)) +
        geom_bar(stat='summary', fun.y = "median") +
        scale_y_continuous(breaks= seq(0, 800000, by=50000), labels = scales::comma) +
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3) +
        geom_hline(yintercept=163000, linetype="dashed", color = "red")+
        theme_bw()+
        theme(axis.text.x = element_text(angle = 45, hjust = 1))+
        yikai_themes+
        yikai_labels+
       theme(panel.grid.major.x  = element_blank())+
       labs(title = "Median SalePrice by Neighborhood")
```



]]

---
class: split-60  

.column.bg-main1[.content.vmiddle[

# Plotting the missingness in that dataset 

###.orange[white lines] indicate missing values in the corresponding variable
### Turns out some missingness indicate the .orange[absence] of that feature (e.g. a pool or fense), which we could impurte as a .pink[None] type 
### some are data misentry and we need to .yellow[find them out!] 


]]

.column.bg-main5[.content.vmiddle.center[
<center>

```{r, echo = F}
  all[, 1:ncol(all)] %>%
    is.na %>%
    melt %>%
    group_by(Var2) %>%
    summarise(n_missing = sum(value)) -> missing_cols
```




```{r, message= F, warning=F, echo = F, out.width = "90%" , dpi = 300, out.height= "75%"}
  all[, 1:ncol(all)] %>%
    is.na %>%
    melt %>%
    filter(Var2 %in% missing_cols$Var2[missing_cols$n_missing > 0]) %>%
  ggplot(data = .,
         aes(x = Var2,
             y = Var1)) +
    geom_raster(aes(fill = value)) +
    scale_fill_grey(name = "",
                    labels = c("Present","Missing")) +
    theme_minimal() +
    theme(axis.text.x  = element_text(angle=90, vjust=0.5)) +
    labs(x = "Variables in Dataset",
         y = "Rows / observations")+
    yikai_themes+
    yikai_labels+
    theme(panel.grid.minor = element_blank())+
    theme(panel.grid.major = element_blank())


  
```


]]

---

class: split-40

.column.bg-main1[.content.vmiddle[
## Among the NA's, some are due to misentry of data

<br>

### e.g. While most NA's in .orange[Garage] related variables indicates a lack of a garage, 2 houses have contradictory values in those variables



]]


.column.bg-main4[.content.vmiddle[


```{r, echo = F, eval = F}
all[!is.na(all$GarageType) & is.na(all$GarageFinish), c('GarageCars', 'GarageArea', 'GarageType', 'GarageCond', 'GarageQual', 'GarageFinish')]
```

```{r, eval = F, echo = T}
GarageCars GarageArea GarageType GarageCond GarageQual GarageFinish
2127          1        360     Detchd       <NA>       <NA>         <NA>
2577         NA         NA     Detchd       <NA>       <NA>         <NA>
```

<br>
Given no other variables could predict the garage variables, I think house 2127 indeed has a garage thus the rest of NA's are imputed with the mode.
```{r, echo = T}
all$GarageCond[2127] <- names(sort(-table(all$GarageCond)))[1]
all$GarageQual[2127] <- names(sort(-table(all$GarageQual)))[1]
all$GarageFinish[2127] <- names(sort(-table(all$GarageFinish)))[1]
```

However, house 2577 does not seem to have a garage but with a wrong input of data in .teal[GarageType]. Thus all garage varialbes have been imputed into values indicating there is no garage. 




]]
---

class: split-50

.column.bg-main1[.content.vmiddle[

# We also need to code the character variables into numbers

## Need to consider if the variables is .orange[categorical] or .teal[ordinal]

### If there is no inherent order within the variable, it will be coded as a factor variable
]]

.column.bg-main4[.content.vmiddle[

### For example, the .orange[heating] variable has the following levels:


```{r, echo = T, eval = F}
   Floor    Floor Furnace
   GasA Gas forced warm air furnace
   GasW Gas hot water or steam heat
   Grav Gravity furnace 
   OthW Hot water or steam heat other than gas
   Wall Wall furnace
```

### It will be coded as:

```{r, echo = T, eval = F}
all$Heating <- as.factor(all$Heating)
table(all$Heating)
```

```{r, echo = T,  eval = F}
1    2    3    4    5 
3    92   857  474  1493
```



]]

---

class: split-50

.column.bg-main1[.content.vmiddle[

# We also need to code the character variables into numbers

## Need to consider if the variables is .orange[categorical] or .teal[ordinal]

### If there is an inherent order within the variable, it will be coded as integers indicating the inherent order
]]

.column.bg-main4[.content.vmiddle[

### For example, the .pink[HeatingQC] variable has the following levels:

```{r, echo = F, eval =F}
heating = c(   'Ex', 'Gd', 'TA', 'Fa', 'Po')
heating_qual = c('Excellent',
   'Good', 'Average/Typical', 
  'Fair', 'Poor')
heating = data.frame(heating = heating, heating_qual = heating_qual)
heating
```

```{r, eval = F, echo = T}
   heating    heating_qual
      Ex       Excellent
      Gd            Good
      TA Average/Typical
      Fa            Fair
      Po            Poor

```

### It will be coded as:

```{r, echo = T, eval =F}
Qualities <- c('None' = 0, 'Po' = 1, 
               'Fa' = 2, 'TA' = 3, 
               'Gd' = 4, 'Ex' = 5)
all$HeatingQC<-as.integer(revalue(all$HeatingQC, Qualities))
```
]]

```{r, eval = F, echo = T}
1    2    3    4    5 
3   92  857  474 1493
```


---


class: split-50

.column.bg-main1[.content.vmiddle[
# Dealing with skewness of response variable
## For example, the response variable SalePrice is highly skewed
## We would log transform this variable

]]


.column.bg-main5[.content.vmiddle[

```{r}
SalePrice <- log(all$SalePrice)
```





]]
---
class: bg-main1, center, middle

# Part 2: Modeling of the data

--- 




